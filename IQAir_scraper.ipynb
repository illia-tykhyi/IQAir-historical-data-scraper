{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9a2a7c-a446-43b0-80d9-16975acc3cf5",
   "metadata": {},
   "source": [
    "# IQAir scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42816420-a3b8-4be5-b12d-7793af87d4b1",
   "metadata": {},
   "source": [
    "This vibe-coded project scrapes high‑quality historical PM2.5 air quality data from IQAir without using the paid internal API.  \n",
    "It collects all PM2.5 columns for a chosen continent, together with:\n",
    "\n",
    "- Rank  \n",
    "- City  \n",
    "- Country  \n",
    "- Time periods, following the current layout on the website, for example:  \n",
    "  `| 2024 | Sty | Lut | Mar | Kwi | Maj | Cze | Lip | Sie | Wrz | Paź | Lis | Gru | 2023 | 2022 | 2021 | 2020 | 2019 | 2018 | 2017 |`\n",
    "\n",
    "These time‑period columns are stored generically as `AQI_1`, `AQI_2`, … in the raw output, where:\n",
    "- `AQI_1` corresponds to the current full year (e.g. 2024)  \n",
    "- `AQI_2`–`AQI_13` correspond to months from January to December of the current year  \n",
    "- `AQI_14+` correspond to previous full years (e.g. 2023, 2022, …)\n",
    "\n",
    "A separate cleaning step converts these technical names into readable labels like `AQI_2024`, `Jan_2024`, …, adjusting automatically when the website’s current year changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9516bfcd-031f-4257-9b40-2485460e1194",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa2196e-c776-4424-b59d-f9a2c53cb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5e1ca-c785-4ec7-808b-3c37d1c659f9",
   "metadata": {},
   "source": [
    "## Scraping part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54b660-7f52-40f0-8eee-4708d8e8dc7b",
   "metadata": {},
   "source": [
    "Matters to adjust (in **bold**):\n",
    "- BASE_URL = \"**https://www.iqair.com/pl/world-most-polluted-cities**\"\n",
    "- CONTINENT_ID = \"**59af92ac3e70001c1bd78e52**\" (accesed inside URL - https// www. iqair .com/pl/world-most-polluted-cities?continent=**59af92ac3e70001c1bd78e52** <- like here)\n",
    "- data = scrape_all_pages(max_pages=**10**) (change number of pages to scrape)\n",
    "- output_file = '**CHECK.csv**' (name an output file which automatically will be saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cd84ea-51c3-44d1-b8fb-6c498be29496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQAir Data Scraper (all columns)\n",
      " \n",
      "Starting to scrape 1 pages...\n",
      "Scraping page 1/1...\n",
      "  Found 50 cities on page 1\n",
      "\n",
      "Scraping completed!\n",
      "Total cities: 50\n",
      "AQI columns: 20\n",
      "Data saved to: CHECK.csv\n",
      "\n",
      "First 3 rows (all columns):\n",
      "   Rank          City             Country  AQI_1  AQI_2  AQI_3  AQI_4  AQI_5  \\\n",
      "0     1    Novi Pazar              Serbia   38.8   94.2   65.0   38.5   21.7   \n",
      "1     2  Bijelo Polje          Montenegro   32.4   77.4   53.8   32.4   12.3   \n",
      "2     3      Pljevlja          Montenegro   31.1   77.1   61.0   28.0   17.3   \n",
      "3     4       Valjevo              Serbia   31.0   73.1   44.3   29.3   17.8   \n",
      "4     5      Sarajevo  Bosnia Herzegovina   30.8   62.9   37.2   19.3   14.5   \n",
      "\n",
      "   AQI_6  AQI_7  ...  AQI_11  AQI_12  AQI_13  AQI_14  AQI_15  AQI_16  AQI_17  \\\n",
      "0   13.9   14.4  ...    33.1    73.9    83.6    28.8    41.7    46.6     NaN   \n",
      "1    8.6   11.1  ...    22.5    63.9    57.7    34.5    39.6     NaN     NaN   \n",
      "2    9.9   10.5  ...    21.3    50.8    51.0    40.1    34.5     NaN     NaN   \n",
      "3   13.3   14.9  ...    30.5    60.5    61.0    28.0    31.8    34.7    41.5   \n",
      "4    9.8   11.5  ...    23.7    75.8    69.0    28.6    32.4    29.7    42.5   \n",
      "\n",
      "   AQI_18  AQI_19  AQI_20  \n",
      "0     NaN     NaN     NaN  \n",
      "1     NaN     NaN     NaN  \n",
      "2     NaN     NaN     NaN  \n",
      "3    37.9     NaN     NaN  \n",
      "4    34.1    38.4     NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://www.iqair.com/pl/world-most-polluted-cities\"\n",
    "CONTINENT_ID = \"59af92ac3e70001c1bd78e52\"   \n",
    "\n",
    "def scrape_iqair_page(page_number):\n",
    "    \n",
    "    \"\"\"Scrape a single page of air quality data\"\"\"\n",
    "\n",
    "    url = f\"{BASE_URL}?continent={CONTINENT_ID}&page={page_number}\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        data = []\n",
    "\n",
    "        # Find the table body\n",
    "        tbody = soup.find('tbody')\n",
    "\n",
    "        if tbody:\n",
    "            # Find all table rows\n",
    "            rows = tbody.find_all('tr', class_=lambda x: x and 'border-app-gray-200' in x)\n",
    "\n",
    "            for row in rows:\n",
    "                # Get all td elements\n",
    "                cells = row.find_all('td')\n",
    "\n",
    "                if len(cells) >= 15:  # Ensure we have enough columns\n",
    "                    # Extract rank (first column)\n",
    "                    rank = cells[0].get_text(strip=True)\n",
    "\n",
    "                    # Extract city and country (second column)\n",
    "                    city_country_cell = cells[1]\n",
    "                    city_country_div = city_country_cell.find('div', class_='text-input-text')\n",
    "\n",
    "                    if city_country_div:\n",
    "                        text_content = city_country_div.get_text(strip=True)\n",
    "                        # Split by comma to separate city and country\n",
    "                        parts = [p.strip().strip('\"').strip(',') for p in text_content.split(',')]\n",
    "                        city = parts[0] if len(parts) > 0 else ''\n",
    "                        country = parts[1] if len(parts) > 1 else ''\n",
    "                    else:\n",
    "                        city = ''\n",
    "                        country = ''\n",
    "\n",
    "                    aqi_values = {}\n",
    "                for i, cell in enumerate(cells[2:], start=1):\n",
    "                    aqi_div = cell.find('div')\n",
    "                    value = aqi_div.get_text(strip=True) if aqi_div else cell.get_text(strip=True)\n",
    "                    aqi_values[f'AQI_{i}'] = value \n",
    "\n",
    "                    # Store ALL data (no filtering)\n",
    "                row_data = {\n",
    "                    'Rank': rank,\n",
    "                    'City': city,\n",
    "                    'Country': country,\n",
    "                    **aqi_values \n",
    "                }\n",
    "                \n",
    "                # Only add if we have city/country (basic validation)\n",
    "                if city and country:\n",
    "                    data.append(row_data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching page {page_number}: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_all_pages(max_pages=100):\n",
    "\n",
    "    \"\"\"Scrape all pages of data\"\"\"\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    print(f\"Starting to scrape {max_pages} pages...\")\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Scraping page {page}/{max_pages}...\")\n",
    "\n",
    "        page_data = scrape_iqair_page(page)\n",
    "        all_data.extend(page_data)\n",
    "\n",
    "        print(f\"  Found {len(page_data)} cities on page {page}\")\n",
    "\n",
    "        # Be respectful - add delay between requests\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Stop if no data found (reached end)\n",
    "        if not page_data:\n",
    "            print(f\"No data found on page {page}. Stopping.\")\n",
    "            break\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"Main function to run the scraper\"\"\"\n",
    "\n",
    "    print(\"IQAir Data Scraper (all columns)\")\n",
    "    print(\" \")\n",
    "\n",
    "    # Scrape all pages\n",
    "    data = scrape_all_pages(max_pages=10)\n",
    "\n",
    "    if data:\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Convert ALL AQI columns to numeric (keep NaN for blanks)\n",
    "        aqi_cols = [col for col in df.columns if col.startswith('AQI_')]\n",
    "        for col in aqi_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')  # Blanks become NaN\n",
    "        \n",
    "        # Sort by rank only\n",
    "        df['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n",
    "        df = df.sort_values('Rank').reset_index(drop=True)\n",
    "        \n",
    "        # Save ALL data\n",
    "        output_file = 'CHECK.csv'\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\nScraping completed!\")\n",
    "        print(f\"Total cities: {len(df)}\")\n",
    "        print(f\"AQI columns: {len(aqi_cols)}\")\n",
    "        print(f\"Data saved to: {output_file}\")\n",
    "        print(\"\\nFirst 3 rows (all columns):\")\n",
    "        print(df.head(5))\n",
    "\n",
    "    else:\n",
    "        print(\"No data was scraped. The website structure may have changed or requires JavaScript rendering.\")\n",
    "        print(\"Consider using Selenium with BeautifulSoup for JavaScript-heavy sites.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41885c-0316-4dfe-8f04-7e81bfc70f46",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87ad91-44fa-4f90-9d29-b5b9ed9810e9",
   "metadata": {},
   "source": [
    "- df = pd.read_csv(r"**C:\Users\USER\Downloads\CHECK.csv**") (insert the path to scraped data)\n",
    "- CURRENT_YEAR = **2024** (change this to match the website's current year (AQI_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0acdf281-2c2d-4300-9bc4-b33b5c2b197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rename mapping:\n",
      "  AQI_1 -> AQI_2024\n",
      "  AQI_2 -> Jan_2024\n",
      "  AQI_3 -> Feb_2024\n",
      "  AQI_4 -> Mar_2024\n",
      "  AQI_5 -> Apr_2024\n",
      "  AQI_6 -> May_2024\n",
      "  AQI_7 -> Jun_2024\n",
      "  AQI_8 -> Jul_2024\n",
      "  AQI_9 -> Aug_2024\n",
      "  AQI_10 -> Sep_2024\n",
      "  AQI_11 -> Oct_2024\n",
      "  AQI_12 -> Nov_2024\n",
      "  AQI_13 -> Dec_2024\n",
      "  AQI_14 -> AQI_2023\n",
      "  AQI_15 -> AQI_2022\n",
      "  AQI_16 -> AQI_2021\n",
      "  AQI_17 -> AQI_2020\n",
      "  AQI_18 -> AQI_2019\n",
      "  AQI_19 -> AQI_2018\n",
      "  AQI_20 -> AQI_2017\n",
      "   Rank          City             Country  AQI_2024  Jan_2024  Feb_2024  \\\n",
      "0     1    Novi Pazar              Serbia      38.8      94.2      65.0   \n",
      "1     2  Bijelo Polje          Montenegro      32.4      77.4      53.8   \n",
      "2     3      Pljevlja          Montenegro      31.1      77.1      61.0   \n",
      "3     4       Valjevo              Serbia      31.0      73.1      44.3   \n",
      "4     5      Sarajevo  Bosnia Herzegovina      30.8      62.9      37.2   \n",
      "\n",
      "   Mar_2024  Apr_2024  May_2024  Jun_2024  ...  Oct_2024  Nov_2024  Dec_2024  \\\n",
      "0      38.5      21.7      13.9      14.4  ...      33.1      73.9      83.6   \n",
      "1      32.4      12.3       8.6      11.1  ...      22.5      63.9      57.7   \n",
      "2      28.0      17.3       9.9      10.5  ...      21.3      50.8      51.0   \n",
      "3      29.3      17.8      13.3      14.9  ...      30.5      60.5      61.0   \n",
      "4      19.3      14.5       9.8      11.5  ...      23.7      75.8      69.0   \n",
      "\n",
      "   AQI_2023  AQI_2022  AQI_2021  AQI_2020  AQI_2019  AQI_2018  AQI_2017  \n",
      "0      28.8      41.7      46.6       NaN       NaN       NaN       NaN  \n",
      "1      34.5      39.6       NaN       NaN       NaN       NaN       NaN  \n",
      "2      40.1      34.5       NaN       NaN       NaN       NaN       NaN  \n",
      "3      28.0      31.8      34.7      41.5      37.9       NaN       NaN  \n",
      "4      28.6      32.4      29.7      42.5      34.1      38.4       NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the file\n",
    "df = pd.read_csv(r\"C:\\Users\\Elijah\\Downloads\\CHECK.csv\")\n",
    "\n",
    "# Set the current year (AQI_1 corresponds to this year) - ADJUST THIS WHEN WEBSITE CHANGES\n",
    "CURRENT_YEAR = 2024  \n",
    "\n",
    "# Get all AQI columns\n",
    "aqi_columns = [col for col in df.columns if col.startswith('AQI_')]\n",
    "\n",
    "# Create mapping dictionary dynamically\n",
    "rename_dict = {}\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "for i, col in enumerate(aqi_columns, start=1):\n",
    "    if i == 1:\n",
    "        # AQI_1 = full current year\n",
    "        new_name = f\"AQI_{CURRENT_YEAR}\"\n",
    "    elif 2 <= i <= 13:\n",
    "        # AQI_2–AQI_13 = Jan–Dec of current year\n",
    "        month_abbr = months[i - 2]       # 2 -> Jan, 3 -> Feb, ..., 13 -> Dec\n",
    "        new_name = f\"{month_abbr}_{CURRENT_YEAR}\"\n",
    "    else:\n",
    "        # AQI_14+ = previous full years\n",
    "        years_back = i - 13             # 14 -> 1 year back, 15 -> 2 years back, ...\n",
    "        year = CURRENT_YEAR - years_back\n",
    "        new_name = f\"AQI_{year}\"\n",
    "\n",
    "    rename_dict[col] = new_name\n",
    "\n",
    "# Apply renaming\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# Keep only relevant columns\n",
    "columns_to_keep = ['Rank', 'City', 'Country'] + list(rename_dict.values())\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "print(\"Rename mapping:\")\n",
    "for old, new in rename_dict.items():\n",
    "    print(f\"  {old} -> {new}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b40a3-4c1f-4d09-acc5-80f4b5f1eb33",
   "metadata": {},
   "source": [
    "### Save clean version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e85387-b75f-4421-9459-ca8748be4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clean dataset\n",
    "df.to_csv(\"CLEAN_CHECK.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c40647-c5e1-453d-a767-00573da35e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
